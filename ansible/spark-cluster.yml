---
- name: Setup Spark Cluster Base
  hosts: all
  become: yes

  vars:
    service_user: "hadoop"
    java_version: "openjdk-17-jdk"
    hadoop_version: "3.3.6"
    spark_version: "3.5.0"
    install_dir: "/opt"
    hadoop_home: "{{ install_dir }}/hadoop-{{ hadoop_version }}"
    spark_home: "{{ install_dir }}/spark-{{ spark_version }}-bin-hadoop3"

  tasks:
    - name: Create Hadoop service user
      user:
        name: "{{ service_user }}"
        shell: /bin/bash
        home: "/home/{{ service_user }}"
        state: present

    - name: Update apt cache
      apt:
        update_cache: yes
        cache_valid_time: 3600

    - name: Install Java and Dependencies
      apt:
        name:
          - "{{ java_version }}"
          - wget
          - ssh
          - rsync
          - git
          - python3-pip
        state: present

    - name: Download Hadoop
      get_url:
        url: "https://archive.apache.org/dist/hadoop/common/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz"
        dest: "/tmp/hadoop.tar.gz"
        mode: '0644'

    - name: Extract Hadoop
      unarchive:
        src: "/tmp/hadoop.tar.gz"
        dest: "{{ install_dir }}"
        remote_src: yes
        creates: "{{ hadoop_home }}"
        owner: "{{ service_user }}"
        group: "{{ service_user }}"

    - name: Download Spark
      get_url:
        url: "https://archive.apache.org/dist/spark/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-hadoop3.tgz"
        dest: "/tmp/spark.tar.gz"
        mode: '0644'

    - name: Extract Spark
      unarchive:
        src: "/tmp/spark.tar.gz"
        dest: "{{ install_dir }}"
        remote_src: yes
        creates: "{{ spark_home }}"
        owner: "{{ service_user }}"
        group: "{{ service_user }}"

    - name: Fix permissions recursively
      file:
        path: "{{ item }}"
        state: directory
        recurse: yes
        owner: "{{ service_user }}"
        group: "{{ service_user }}"
      loop:
        - "{{ hadoop_home }}"
        - "{{ spark_home }}"

    - name: Create logs directories
      file:
        path: "{{ item }}/logs"
        state: directory
        owner: "{{ service_user }}"
        group: "{{ service_user }}"
        mode: '0755'
      loop:
        - "{{ hadoop_home }}"
        - "{{ spark_home }}"

    - name: Set global environment variables
      copy:
        dest: /etc/profile.d/spark.sh
        mode: '0755'
        content: |
          export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
          export HADOOP_HOME={{ hadoop_home }}
          export SPARK_HOME={{ spark_home }}
          export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOME/sbin

- name: Configure SSH for Hadoop User
  hosts: all
  become: yes
  vars:
    service_user: "hadoop"

  tasks:
    - name: Create .ssh directory for service user
      file:
        path: "/home/{{ service_user }}/.ssh"
        state: directory
        owner: "{{ service_user }}"
        group: "{{ service_user }}"
        mode: '0700'

    - name: Generate SSH key pair
      openssh_keypair:
        path: "/home/{{ service_user }}/.ssh/id_rsa"
        type: rsa
        size: 2048
        state: present
        owner: "{{ service_user }}"
        group: "{{ service_user }}"
        force: no

    - name: Configure SSH config
      blockinfile:
        path: "/home/{{ service_user }}/.ssh/config"
        create: yes
        owner: "{{ service_user }}"
        group: "{{ service_user }}"
        mode: '0600'
        block: |
          Host *
              StrictHostKeyChecking no
              UserKnownHostsFile=/dev/null

    - name: Fetch public key
      shell: "cat /home/{{ service_user }}/.ssh/id_rsa.pub"
      register: ssh_keys
      changed_when: false

- name: Distribute authorized_keys
  hosts: all
  become: yes
  vars:
    service_user: "hadoop"
  
  tasks:
    - name: Add keys to authorized_keys
      authorized_key:
        user: "{{ service_user }}"
        key: "{{ item[0] }}"
        state: present
      delegate_to: "{{ item[1] }}"
      with_nested:
        - "{{ play_hosts | map('extract', hostvars, 'ssh_keys') | map(attribute='stdout') | list }}"
        - "{{ play_hosts }}"

- name: Configure Hadoop environment
  hosts: all
  become: yes
  vars:
    service_user: "hadoop"
    hadoop_home: "/opt/hadoop-3.3.6"
    master_ip: "{{ hostvars[groups['master'][0]]['ansible_default_ipv4']['address'] }}"
  
  tasks:
    - name: Set JAVA_HOME in hadoop-env.sh
      lineinfile:
        path: "{{ hadoop_home }}/etc/hadoop/hadoop-env.sh"
        regexp: '^export JAVA_HOME='
        line: 'export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64'
        create: yes
        owner: "{{ service_user }}"

    - name: Configure core-site.xml
      copy:
        dest: "{{ hadoop_home }}/etc/hadoop/core-site.xml"
        owner: "{{ service_user }}"
        content: |
          <configuration>
            <property>
              <name>hadoop.tmp.dir</name>
              <value>/tmp/hadoop</value>
            </property>
            <property>
              <name>fs.defaultFS</name>
              <value>hdfs://{{ master_ip }}:9000</value>
            </property>
          </configuration>

    - name: Create Hadoop tmp directory
      file:
        path: /tmp/hadoop
        state: directory
        owner: "{{ service_user }}"
        group: "{{ service_user }}"
        mode: '0755'

- name: Configure Master Node and Start Services
  hosts: master
  become: yes
  vars:
    service_user: "hadoop"
    hadoop_home: "/opt/hadoop-3.3.6"
    spark_home: "/opt/spark-3.5.0-bin-hadoop3"

  tasks:
    - name: Configure hdfs-site.xml
      copy:
        dest: "{{ hadoop_home }}/etc/hadoop/hdfs-site.xml"
        owner: "{{ service_user }}"
        content: |
          <configuration>
            <property>
              <name>dfs.replication</name>
              <value>1</value>
            </property>
            <property>
              <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
              <value>false</value>
            </property>
            <property>
              <name>dfs.permissions.enabled</name>
              <value>false</value>
            </property>
          </configuration>

    - name: Configure Hadoop workers
      copy:
        dest: "{{ hadoop_home }}/etc/hadoop/workers"
        owner: "{{ service_user }}"
        content: |
          {% for host in groups['workers'] %}
          {{ hostvars[host]['inventory_hostname'] }}
          {% endfor %}

    - name: Configure Spark workers
      copy:
        dest: "{{ spark_home }}/conf/workers"
        owner: "{{ service_user }}"
        content: |
          {% for host in groups['workers'] %}
          {{ hostvars[host]['inventory_hostname'] }}
          {% endfor %}

    - name: Configure Spark environment
      copy:
        dest: "{{ spark_home }}/conf/spark-env.sh"
        owner: "{{ service_user }}"
        mode: '0755'
        content: |
          export SPARK_MASTER_HOST={{ hostvars[groups['master'][0]]['ansible_default_ipv4']['address'] }}
          export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64

    - name: Format HDFS namenode
      become_user: "{{ service_user }}"
      shell: "{{ hadoop_home }}/bin/hdfs namenode -format -force"
      args:
        creates: /tmp/hadoop/dfs/name
      environment:
        JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64
        HADOOP_HOME: "{{ hadoop_home }}"

    - name: Start HDFS Services
      become_user: "{{ service_user }}"
      shell: "{{ hadoop_home }}/sbin/start-dfs.sh"
      environment:
        JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64
        HADOOP_HOME: "{{ hadoop_home }}"

    - name: Start Spark Services
      become_user: "{{ service_user }}"
      shell: "{{ spark_home }}/sbin/start-all.sh"
      environment:
        JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64
        SPARK_HOME: "{{ spark_home }}"

- name: Deploy Application on Edge
  hosts: edge
  become: yes
  vars:
    service_user: "hadoop"
    spark_home: "/opt/spark-3.5.0-bin-hadoop3"
    hadoop_home: "/opt/hadoop-3.3.6"
    app_dir: "/opt/spark-app"
    master_ip: "{{ hostvars[groups['master'][0]]['ansible_default_ipv4']['address'] }}"

  tasks:
    - name: Clone Git repository
      git:
        repo: 'https://github.com/Zouloux11/ProjectCLOUD_GCP.git'
        dest: "{{ app_dir }}"
        force: yes

    - name: Set permissions for app directory (RWX for everyone)
      file:
        path: "{{ app_dir }}"
        state: directory
        recurse: yes
        mode: '0777'

    - name: Make scripts executable
      file:
        path: "{{ app_dir }}/spark-app/{{ item }}"
        mode: '0777'
      loop:
        - comp.sh
        - run.sh

    - name: Update run.sh with correct master IP
      lineinfile:
        path: "{{ app_dir }}/spark-app/run.sh"
        regexp: 'spark://master:7077'
        line: "    --master spark://{{ master_ip }}:7077 \\"
      
    - name: Compile WordCount
      shell: ./comp.sh
      args:
        chdir: "{{ app_dir }}/spark-app"
      environment:
        SPARK_HOME: "{{ spark_home }}"
        HADOOP_HOME: "{{ hadoop_home }}"
        JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64