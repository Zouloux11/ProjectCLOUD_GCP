---
- name: Setup Spark Cluster
  hosts: all
  become: yes

  vars:
    java_version: "openjdk-17-jdk"
    hadoop_version: "3.3.6"
    spark_version: "3.5.0"

    install_dir: "/opt"
    hadoop_home: "{{ install_dir }}/hadoop-{{ hadoop_version }}"
    spark_home: "{{ install_dir }}/spark-{{ spark_version }}-bin-hadoop3"

  tasks:

    - name: Update apt cache
      apt:
        update_cache: yes

    - name: Install Java 17
      apt:
        name: "{{ java_version }}"
        state: present

    - name: Install required packages
      apt:
        name:
          - wget
          - ssh
          - rsync
          - git
        state: present

    - name: Download Hadoop
      get_url:
        url: "https://archive.apache.org/dist/hadoop/common/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz"
        dest: "/tmp/hadoop.tar.gz"

    - name: Create install directory
      file:
        path: "{{ install_dir }}"
        state: directory
        owner: root
        group: root
        mode: '0755'

    - name: Extract Hadoop
      unarchive:
        src: "/tmp/hadoop.tar.gz"
        dest: "{{ install_dir }}"
        remote_src: yes
        creates: "{{ hadoop_home }}"

    - name: Download Spark
      get_url:
        url: "https://archive.apache.org/dist/spark/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-hadoop3.tgz"
        dest: "/tmp/spark.tar.gz"

    - name: Extract Spark
      unarchive:
        src: "/tmp/spark.tar.gz"
        dest: "{{ install_dir }}"
        remote_src: yes
        creates: "{{ spark_home }}"

    - name: Set global environment variables for all users
      copy:
        dest: /etc/profile.d/spark.sh
        mode: '0755'
        content: |
          export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
          export HADOOP_HOME={{ hadoop_home }}
          export SPARK_HOME={{ spark_home }}
          export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOME/sbin

- name: Configure Hadoop environment on all nodes
  hosts: all
  become: yes

  vars:
    hadoop_home: "/opt/hadoop-3.3.6"
    master_internal_ip: "{{ hostvars[groups['master'][0]]['ansible_default_ipv4']['address'] }}"
  tasks:
    - name: Configure Hadoop environment
      copy:
        dest: "{{ hadoop_home }}/etc/hadoop/hadoop-env.sh"
        content: |
          export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
        mode: '0755'

    - name: Configure Hadoop core-site.xml on all nodes
      copy:
        dest: "{{ hadoop_home }}/etc/hadoop/core-site.xml"
        content: |
          <configuration>
            <property>
              <name>hadoop.tmp.dir</name>
              <value>/tmp/hadoop</value>
            </property>
            <property>
              <name>fs.defaultFS</name>
              <value>hdfs://{{ master_internal_ip }}:9000</value>
            </property>
          </configuration>

- name: Configure Master Node
  hosts: master
  become: yes
  vars:
    hadoop_home: "/opt/hadoop-3.3.6"
    spark_home: "/opt/spark-3.5.0-bin-hadoop3"

  tasks:

    - name: Configure Hadoop hdfs-site.xml
      copy:
        dest: "{{ hadoop_home }}/etc/hadoop/hdfs-site.xml"
        content: |
          <configuration>
            <property>
              <name>dfs.replication</name>
              <value>1</value>
            </property>
            <property>
              <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
              <value>false</value>
            </property>
          </configuration>

    - name: Configure Hadoop workers
      copy:
        dest: "{{ hadoop_home }}/etc/hadoop/workers"
        content: |
          {% for host in groups['workers'] %}
          {{ hostvars[host]['inventory_hostname'] }}
          {% endfor %}

    - name: Configure Spark workers
      copy:
        dest: "{{ spark_home }}/conf/workers"
        content: |
          {% for host in groups['workers'] %}
          {{ hostvars[host]['inventory_hostname'] }}
          {% endfor %}

    - name: Create Hadoop tmp directory
      file:
        path: /tmp/hadoop
        state: directory
        owner: root
        group: root
        mode: '0755'

    - name: Format HDFS namenode
      shell: "{{ hadoop_home }}/bin/hdfs namenode -format -force"
      args:
        creates: /tmp/hadoop/dfs/name

- name: Deploy and Compile Application
  hosts: edge
  become: yes

  vars:
    spark_home: "/opt/spark-3.5.0-bin-hadoop3"
    hadoop_home: "/opt/hadoop-3.3.6"
    app_dir: "/opt/spark-app"

  tasks:

    - name: Clone Git repository
      git:
        repo: 'https://github.com/Zouloux11/ProjectCLOUD_GCP.git'
        dest: "{{ app_dir }}"
        force: yes

    - name: Make scripts executable
      file:
        path: "{{ app_dir }}/spark-app/{{ item }}"
        mode: '0755'
      loop:
        - comp.sh
        - run.sh
      ignore_errors: yes

    - name: Update run.sh with correct master IP
      lineinfile:
        path: "{{ app_dir }}/spark-app/run.sh"
        regexp: 'spark://master:7077'
        line: "    --master spark://{{ hostvars[groups['master'][0]]['ansible_default_ipv4']['address'] }}:7077 \\"
      ignore_errors: yes
      
    - name: Compile WordCount
      shell: ./comp.sh
      args:
        chdir: "{{ app_dir }}/spark-app"
      environment:
        SPARK_HOME: "{{ spark_home }}"
        HADOOP_HOME: "{{ hadoop_home }}"
        JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64
