---
- name: Setup Spark Cluster
  hosts: all
  become: no
  vars:
    java_version: "openjdk-17-jdk"
    hadoop_version: "3.3.6"
    spark_version: "3.5.0"
    install_dir: "/opt"
    hadoop_home: "{{ install_dir }}/hadoop-{{ hadoop_version }}"
    spark_home: "{{ install_dir }}/spark-{{ spark_version }}-bin-hadoop3"
    
  tasks:
    - name: Update apt cache
      apt:
        update_cache: yes
      become: yes
    
    - name: Install Java 17
      apt:
        name: "{{ java_version }}"
        state: present
      become: yes
    
    - name: Install required packages
      apt:
        name:
          - wget
          - ssh
          - rsync
          - git
        state: present
      become: yes
    
    - name: Download Hadoop
      get_url:
        url: "https://archive.apache.org/dist/hadoop/common/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz"
        dest: "/tmp/hadoop.tar.gz"
    
    - name: Create install directory
      file:
        path: "{{ install_dir }}"
        state: directory
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: '0755'
      become: yes
    
    - name: Extract Hadoop
      unarchive:
        src: "/tmp/hadoop.tar.gz"
        dest: "{{ install_dir }}"
        remote_src: yes
        creates: "{{ hadoop_home }}"
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
    
    - name: Download Spark
      get_url:
        url: "https://archive.apache.org/dist/spark/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-hadoop3.tgz"
        dest: "/tmp/spark.tar.gz"
    
    - name: Extract Spark
      unarchive:
        src: "/tmp/spark.tar.gz"
        dest: "{{ install_dir }}"
        remote_src: yes
        creates: "{{ spark_home }}"
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
    
    - name: Set environment variables in user bashrc
      lineinfile:
        path: "~/.bashrc"
        line: "{{ item }}"
      loop:
        - "export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64"
        - "export HADOOP_HOME={{ hadoop_home }}"
        - "export SPARK_HOME={{ spark_home }}"
        - "export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOME/sbin"
    
    - name: Create SSH key if not exists
      openssh_keypair:
        path: "~/.ssh/id_rsa"
        type: rsa
        size: 2048
        state: present
        force: no
    
    - name: Read SSH public key
      slurp:
        src: "~/.ssh/id_rsa.pub"
      register: ssh_pub_key
    
    - name: Add SSH key to authorized_keys
      authorized_key:
        user: "{{ ansible_user }}"
        key: "{{ ssh_pub_key['content'] | b64decode }}"
        state: present
    
    - name: Configure SSH to skip host checking
      blockinfile:
        path: "~/.ssh/config"
        create: yes
        mode: '0600'
        block: |
          Host *
              StrictHostKeyChecking no
              UserKnownHostsFile=/dev/null

- name: Configure SSH between nodes
  hosts: all
  become: no
  tasks:
    - name: Gather master public key
      slurp:
        src: "~/.ssh/id_rsa.pub"
      register: master_key
      delegate_to: "{{ groups['master'][0] }}"
      run_once: true
    
    - name: Add master key to all nodes authorized_keys
      authorized_key:
        user: "{{ ansible_user }}"
        key: "{{ master_key['content'] | b64decode }}"
        state: present

- name: Gather facts for IP addresses
  hosts: all
  become: no
  tasks:
    - name: Gather network facts
      setup:
        gather_subset:
          - network

- name: Configure Hadoop environment on all nodes
  hosts: all
  become: no
  vars:
    hadoop_home: "/opt/hadoop-3.3.6"
    master_internal_ip: "{{ hostvars[groups['master'][0]]['ansible_default_ipv4']['address'] }}"
  tasks:
    - name: Configure Hadoop environment
      copy:
        dest: "{{ hadoop_home }}/etc/hadoop/hadoop-env.sh"
        content: |
          export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
        mode: '0755'
    
    - name: Configure Hadoop core-site.xml on all nodes
      copy:
        dest: "{{ hadoop_home }}/etc/hadoop/core-site.xml"
        content: |
          <configuration>
            <property>
              <name>hadoop.tmp.dir</name>
              <value>/tmp/hadoop</value>
            </property>
            <property>
              <name>fs.defaultFS</name>
              <value>hdfs://{{ master_internal_ip }}:9000</value>
            </property>
          </configuration>

- name: Configure Master Node
  hosts: master
  become: no
  vars:
    hadoop_home: "/opt/hadoop-3.3.6"
    spark_home: "/opt/spark-3.5.0-bin-hadoop3"
    
  tasks:
    - name: Configure Hadoop hdfs-site.xml
      copy:
        dest: "{{ hadoop_home }}/etc/hadoop/hdfs-site.xml"
        content: |
          <configuration>
            <property>
              <name>dfs.replication</name>
              <value>1</value>
            </property>
            <property>
              <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
              <value>false</value>
            </property>
          </configuration>
    
    - name: Configure Hadoop workers
      copy:
        dest: "{{ hadoop_home }}/etc/hadoop/workers"
        content: |
          {% for host in groups['workers'] %}
          {{ hostvars[host]['inventory_hostname'] }}
          {% endfor %}
    
    - name: Configure Spark workers
      copy:
        dest: "{{ spark_home }}/conf/workers"
        content: |
          {% for host in groups['workers'] %}
          {{ hostvars[host]['inventory_hostname'] }}
          {% endfor %}
    
    - name: Configure Spark environment
      copy:
        dest: "{{ spark_home }}/conf/spark-env.sh"
        content: |
          export SPARK_MASTER_HOST={{ hostvars[groups['master'][0]]['ansible_default_ipv4']['address'] }}
          export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64
        mode: '0755'
    
    - name: Create Hadoop tmp directory
      file:
        path: /tmp/hadoop
        state: directory
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: '0755'
      become: yes
    
    - name: Format HDFS namenode
      shell: "{{ hadoop_home }}/bin/hdfs namenode -format -force"
      args:
        creates: /tmp/hadoop/dfs/name
      environment:
        JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64
        HADOOP_HOME: "{{ hadoop_home }}"

- name: Deploy and Compile Application
  hosts: edge
  become: no
  vars:
    spark_home: "/opt/spark-3.5.0-bin-hadoop3"
    hadoop_home: "/opt/hadoop-3.3.6"
    app_dir: "/home/{{ ansible_user }}/spark-app"
    
  tasks:
    - name: Clone Git repository
      git:
        repo: 'https://github.com/Zouloux11/ProjectCLOUD_GCP.git'
        dest: "{{ app_dir }}"
        force: yes
    
    - name: Make scripts executable
      file:
        path: "{{ app_dir }}/spark-app/{{ item }}"
        mode: '0755'
      loop:
        - comp.sh
        - run.sh
      ignore_errors: yes
    
    - name: Update run.sh with correct master IP
      lineinfile:
        path: "{{ app_dir }}/spark-app/run.sh"
        regexp: 'spark://master:7077'
        line: "    --master spark://{{ hostvars[groups['master'][0]]['ansible_default_ipv4']['address'] }}:7077 \\"
      ignore_errors: yes
    
    - name: Compile WordCount
      shell: ./comp.sh
      args:
        chdir: "{{ app_dir }}/spark-app"
      environment:
        SPARK_HOME: "{{ spark_home }}"
        HADOOP_HOME: "{{ hadoop_home }}"
        JAVA_HOME: /usr/lib/jvm/java-17-openjdk-amd64
      ignore_errors: yes